{
  "_from": "@stdlib/nlp-tokenize@^0.0.x",
  "_id": "@stdlib/nlp-tokenize@0.0.4",
  "_inBundle": false,
  "_integrity": "sha512-sXT1Bsdxb47Gun3r9fTRvu9Lu5D8tOQoyJgSHgueoOSR2AeftGlFLwYGlomv/fXerWChMx04nAwCphLdbuuruA==",
  "_location": "/@stdlib/nlp-tokenize",
  "_phantomChildren": {},
  "_requested": {
    "type": "range",
    "registry": true,
    "raw": "@stdlib/nlp-tokenize@^0.0.x",
    "name": "@stdlib/nlp-tokenize",
    "escapedName": "@stdlib%2fnlp-tokenize",
    "scope": "@stdlib",
    "rawSpec": "^0.0.x",
    "saveSpec": null,
    "fetchSpec": "^0.0.x"
  },
  "_requiredBy": [
    "/@stdlib/string-remove-words"
  ],
  "_resolved": "https://registry.npmjs.org/@stdlib/nlp-tokenize/-/nlp-tokenize-0.0.4.tgz",
  "_shasum": "aeabdd2b7b3bdb8e3d4cd106612793195e5491c0",
  "_spec": "@stdlib/nlp-tokenize@^0.0.x",
  "_where": "C:\\Users\\myall\\Downloads\\txt-file-clean-combine\\node_modules\\@stdlib\\string-remove-words",
  "author": {
    "name": "The Stdlib Authors",
    "url": "https://github.com/stdlib-js/stdlib/graphs/contributors"
  },
  "bugs": {
    "url": "https://github.com/stdlib-js/stdlib/issues"
  },
  "bundleDependencies": false,
  "contributors": [
    {
      "name": "The Stdlib Authors",
      "url": "https://github.com/stdlib-js/stdlib/graphs/contributors"
    }
  ],
  "dependencies": {
    "@stdlib/assert-has-own-property": "^0.0.x",
    "@stdlib/assert-is-boolean": "^0.0.x",
    "@stdlib/assert-is-string": "^0.0.x"
  },
  "deprecated": false,
  "description": "Tokenize a string.",
  "devDependencies": {
    "@stdlib/assert-is-array": "^0.0.x",
    "@stdlib/bench": "^0.0.x",
    "@stdlib/string-from-code-point": "^0.0.x",
    "istanbul": "^0.4.1",
    "tap-spec": "5.x.x",
    "tape": "git+https://github.com/kgryte/tape.git#fix/globby"
  },
  "directories": {
    "benchmark": "./benchmark",
    "doc": "./docs",
    "example": "./examples",
    "lib": "./lib",
    "test": "./test"
  },
  "engines": {
    "node": ">=0.10.0",
    "npm": ">2.7.0"
  },
  "funding": {
    "type": "patreon",
    "url": "https://www.patreon.com/athan"
  },
  "homepage": "https://stdlib.io",
  "keywords": [
    "stdlib",
    "nlp",
    "utilities",
    "utility",
    "utils",
    "util",
    "text mining",
    "tokenizer",
    "split",
    "separate",
    "tokens",
    "word"
  ],
  "license": "Apache-2.0",
  "main": "./lib",
  "name": "@stdlib/nlp-tokenize",
  "os": [
    "aix",
    "darwin",
    "freebsd",
    "linux",
    "macos",
    "openbsd",
    "sunos",
    "win32",
    "windows"
  ],
  "repository": {
    "type": "git",
    "url": "git://github.com/stdlib-js/nlp-tokenize.git"
  },
  "scripts": {
    "benchmark": "make benchmark",
    "examples": "make examples",
    "test": "make test",
    "test-cov": "make test-cov"
  },
  "types": "./docs/types",
  "version": "0.0.4"
}
